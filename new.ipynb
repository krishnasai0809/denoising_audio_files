{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 60ms/step - loss: 0.0165 - val_loss: 0.0055\n",
      "Epoch 2/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 63ms/step - loss: 0.0049 - val_loss: 0.0041\n",
      "Epoch 3/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 64ms/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 4/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 60ms/step - loss: 0.0039 - val_loss: 0.0038\n",
      "Epoch 5/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 57ms/step - loss: 0.0036 - val_loss: 0.0037\n",
      "Epoch 6/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 7/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 8/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 9/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 10/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 59ms/step - loss: 0.0033 - val_loss: 0.0034\n",
      "Epoch 11/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 12/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 13/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 60ms/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 14/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 59ms/step - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 15/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 59ms/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 16/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 59ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 17/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 61ms/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 18/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 19/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 60ms/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 20/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 59ms/step - loss: 0.0029 - val_loss: 0.0028\n",
      "Epoch 21/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 59ms/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 22/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - loss: 0.0030 - val_loss: 0.0028\n",
      "Epoch 23/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 59ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 24/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 67ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 25/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 64ms/step - loss: 0.0027 - val_loss: 0.0028\n",
      "Epoch 26/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 63ms/step - loss: 0.0027 - val_loss: 0.0028\n",
      "Epoch 27/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 28/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 66ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 29/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 67ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 30/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 66ms/step - loss: 0.0026 - val_loss: 0.0027\n",
      "Epoch 31/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 32/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - loss: 0.0028 - val_loss: 0.0027\n",
      "Epoch 33/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 67ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 34/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - loss: 0.0026 - val_loss: 0.0027\n",
      "Epoch 35/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 0.0026 - val_loss: 0.0027\n",
      "Epoch 36/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - loss: 0.0026 - val_loss: 0.0027\n",
      "Epoch 37/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - loss: 0.0026 - val_loss: 0.0027\n",
      "Epoch 38/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - loss: 0.0026 - val_loss: 0.0027\n",
      "Epoch 39/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 76ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 40/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - loss: 0.0027 - val_loss: 0.0026\n",
      "Epoch 41/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 42/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 43/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 44/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 45/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - loss: 0.0025 - val_loss: 0.0026\n",
      "Epoch 46/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 78ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 47/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 79ms/step - loss: 0.0025 - val_loss: 0.0026\n",
      "Epoch 48/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 49/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 50/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - loss: 0.0025 - val_loss: 0.0025\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x30352eee0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import librosa\n",
    "\n",
    "# Step 1: Get list of file paths\n",
    "\n",
    "def get_audio_file_paths(directory):\n",
    "    \"\"\"\n",
    "    Get a list of audio file paths from a given directory.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Path to the directory containing .wav audio files.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of full file paths for .wav files in the directory.\n",
    "    \"\"\"\n",
    "    audio_files = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.wav'):\n",
    "            audio_files.append(os.path.join(directory, filename))\n",
    "    return audio_files\n",
    "\n",
    "\n",
    "# Step 2: Preprocess audio files\n",
    "\n",
    "def preprocess_audio(file_path, target_sample_rate=16000, duration=2.0):\n",
    "    \"\"\"\n",
    "    Load and preprocess an audio file (load, resample, normalize, and pad/crop).\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the audio file.\n",
    "        target_sample_rate (int): The sample rate to which audio will be resampled.\n",
    "        duration (float): Target duration (in seconds) for the audio.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Preprocessed audio signal with fixed length.\n",
    "    \"\"\"\n",
    "    # Load audio file\n",
    "    audio, sr = librosa.load(file_path, sr=target_sample_rate)\n",
    "\n",
    "    # Normalize audio to range [-1, 1]\n",
    "    audio = librosa.util.normalize(audio)\n",
    "\n",
    "    # Ensure fixed length (pad or crop)\n",
    "    target_length = int(target_sample_rate * duration)\n",
    "    if len(audio) > target_length:\n",
    "        audio = audio[:target_length]\n",
    "    else:\n",
    "        padding = target_length - len(audio)\n",
    "        audio = np.pad(audio, (0, padding), 'constant')\n",
    "\n",
    "    return audio\n",
    "\n",
    "\n",
    "# Step 3: Get file lists for clean and noisy datasets\n",
    "\n",
    "clean_dir = 'clean_testset_wav'\n",
    "noisy_dir = 'noisy_testset_wav'\n",
    "\n",
    "clean_files = get_audio_file_paths(clean_dir)\n",
    "noisy_files = get_audio_file_paths(noisy_dir)\n",
    "\n",
    "# Step 4: Prepare dataset\n",
    "\n",
    "# Process all pairs into numpy arrays\n",
    "clean_signals = []\n",
    "noisy_signals = []\n",
    "\n",
    "for clean_fp, noisy_fp in zip(clean_files, noisy_files):\n",
    "    clean_audio = preprocess_audio(clean_fp)\n",
    "    noisy_audio = preprocess_audio(noisy_fp)\n",
    "    \n",
    "    clean_signals.append(clean_audio)\n",
    "    noisy_signals.append(noisy_audio)\n",
    "\n",
    "# Convert lists to numpy arrays for training\n",
    "clean_signals = np.array(clean_signals)\n",
    "noisy_signals = np.array(noisy_signals)\n",
    "\n",
    "# Add channel dimension for TensorFlow (batch_size, timesteps, channels)\n",
    "clean_signals = np.expand_dims(clean_signals, -1)\n",
    "noisy_signals = np.expand_dims(noisy_signals, -1)\n",
    "\n",
    "\n",
    "# Step 5: Build Autoencoder Model using TensorFlow\n",
    "\n",
    "def build_autoencoder(input_shape):\n",
    "    \"\"\"\n",
    "    Build an autoencoder model for audio denoising using TensorFlow.\n",
    "\n",
    "    Args:\n",
    "        input_shape (tuple): Shape of input audio (timesteps, channels).\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: Compiled autoencoder model.\n",
    "    \"\"\"\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    x = tf.keras.layers.Conv1D(16, 3, padding='same', activation='relu')(inputs)\n",
    "    x = tf.keras.layers.MaxPooling1D(2, padding='same')(x)\n",
    "    x = tf.keras.layers.Conv1D(8, 3, padding='same', activation='relu')(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(2, padding='same')(x)\n",
    "    x = tf.keras.layers.Conv1D(8, 3, padding='same', activation='relu')(x)\n",
    "    encoded = tf.keras.layers.MaxPooling1D(2, padding='same')(x)\n",
    "\n",
    "    # Decoder\n",
    "    x = tf.keras.layers.Conv1D(8, 3, padding='same', activation='relu')(encoded)\n",
    "    x = tf.keras.layers.UpSampling1D(2)(x)\n",
    "    x = tf.keras.layers.Conv1D(8, 3, padding='same', activation='relu')(x)\n",
    "    x = tf.keras.layers.UpSampling1D(2)(x)\n",
    "    x = tf.keras.layers.Conv1D(16, 3, padding='same', activation='relu')(x)\n",
    "    x = tf.keras.layers.UpSampling1D(2)(x)\n",
    "    decoded = tf.keras.layers.Conv1D(1, 3, padding='same', activation='tanh')(x)\n",
    "\n",
    "    # Model\n",
    "    autoencoder = tf.keras.Model(inputs, decoded)\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    return autoencoder\n",
    "\n",
    "\n",
    "# Step 6: Instantiate and train the autoencoder\n",
    "\n",
    "input_shape = (clean_signals.shape[1], 1)\n",
    "autoencoder = build_autoencoder(input_shape)\n",
    "\n",
    "# Train the model\n",
    "autoencoder.fit(noisy_signals, clean_signals, epochs=50, batch_size=16, validation_split=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Denoised audio saved to denoised_result.wav\n"
     ]
    }
   ],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "def denoise_audio(file_path, model, target_sample_rate=16000, duration=10.0, output_path='denoised_output.wav'):\n",
    "    \"\"\"\n",
    "    Denoise a new audio file using the trained autoencoder model and save the output.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the noisy input audio file.\n",
    "        model (tf.keras.Model): Trained autoencoder model.\n",
    "        target_sample_rate (int): Target sample rate (Hz) for audio processing.\n",
    "        duration (float): Duration (in seconds) to which audio will be cropped/padded.\n",
    "        output_path (str): Path to save the denoised output .wav file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Step 1: Preprocess the noisy input audio\n",
    "    noisy_audio = preprocess_audio(file_path, target_sample_rate, duration)\n",
    "    noisy_audio = np.expand_dims(noisy_audio, axis=0)  # Add batch dimension\n",
    "    noisy_audio = np.expand_dims(noisy_audio, axis=-1)  # Add channel dimension\n",
    "\n",
    "    # Step 2: Pass through the model to get denoised output\n",
    "    denoised_audio = model.predict(noisy_audio)\n",
    "\n",
    "    # Step 3: Postprocess - remove extra dimensions\n",
    "    denoised_audio = np.squeeze(denoised_audio)\n",
    "\n",
    "    # Step 4: Save the output audio as .wav file\n",
    "    sf.write(output_path, denoised_audio, target_sample_rate)\n",
    "    print(f\"Denoised audio saved to {output_path}\")\n",
    "\n",
    "# Example usage:\n",
    "new_noisy_file = 'noisy_testset_wav/p232_010.wav'  # replace with your noisy input\n",
    "denoise_audio(new_noisy_file, autoencoder, output_path='denoised_result.wav')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from autoencoder_tuning/audio_denoiser_v2/tuner0.json\n",
      "Best Hyperparameters: {'num_layers': 3, 'filters_0': 64, 'kernel_size_0': 3, 'filters_1': 64, 'kernel_size_1': 3, 'learning_rate': 0.0001562434526149395, 'filters_2': 64, 'kernel_size_2': 5, 'filters_3': 16, 'kernel_size_3': 5}\n",
      "Epoch 1/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 486ms/step - loss: 0.0147 - val_loss: 0.0041\n",
      "Epoch 2/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 527ms/step - loss: 0.0038 - val_loss: 0.0033\n",
      "Epoch 3/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 563ms/step - loss: 0.0032 - val_loss: 0.0029\n",
      "Epoch 4/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 577ms/step - loss: 0.0027 - val_loss: 0.0026\n",
      "Epoch 5/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 581ms/step - loss: 0.0025 - val_loss: 0.0024\n",
      "Epoch 6/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 587ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 7/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 615ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 8/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 696ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 9/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 746ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 10/10\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 744ms/step - loss: 0.0022 - val_loss: 0.0022\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x322392400>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import librosa\n",
    "import keras_tuner as kt\n",
    "\n",
    "# Step 1: Get audio file paths\n",
    "def get_audio_file_paths(directory):\n",
    "    return [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.wav')]\n",
    "\n",
    "# Step 2: Preprocess audio\n",
    "def preprocess_audio(file_path, target_sample_rate=16000, duration=2.0):\n",
    "    audio, sr = librosa.load(file_path, sr=target_sample_rate)\n",
    "    audio = librosa.util.normalize(audio)\n",
    "    target_length = int(target_sample_rate * duration)\n",
    "    if len(audio) > target_length:\n",
    "        audio = audio[:target_length]\n",
    "    else:\n",
    "        padding = target_length - len(audio)\n",
    "        audio = np.pad(audio, (0, padding), 'constant')\n",
    "    return audio\n",
    "\n",
    "# Step 3: Load clean & noisy files\n",
    "clean_dir = 'clean_testset_wav'\n",
    "noisy_dir = 'noisy_testset_wav'\n",
    "clean_files = get_audio_file_paths(clean_dir)\n",
    "noisy_files = get_audio_file_paths(noisy_dir)\n",
    "\n",
    "clean_signals, noisy_signals = [], []\n",
    "for c_fp, n_fp in zip(clean_files, noisy_files):\n",
    "    clean_signals.append(preprocess_audio(c_fp))\n",
    "    noisy_signals.append(preprocess_audio(n_fp))\n",
    "\n",
    "clean_signals = np.expand_dims(np.array(clean_signals), -1)\n",
    "noisy_signals = np.expand_dims(np.array(noisy_signals), -1)\n",
    "\n",
    "\n",
    "# Step 4: Autoencoder Model with Balanced Pool/UpSampling\n",
    "def build_autoencoder(hp):\n",
    "    inputs = tf.keras.layers.Input(shape=(clean_signals.shape[1], 1))\n",
    "\n",
    "    # Force same number of encoder and decoder layers\n",
    "    num_layers = hp.Int('num_layers', min_value=2, max_value=4, step=1)\n",
    "    x = inputs\n",
    "\n",
    "    # Encoder\n",
    "    filter_list = []\n",
    "    for i in range(num_layers):\n",
    "        filters = hp.Int(f'filters_{i}', min_value=16, max_value=64, step=16)\n",
    "        filter_list.append(filters)\n",
    "        kernel_size = hp.Choice(f'kernel_size_{i}', [3, 5])\n",
    "        x = tf.keras.layers.Conv1D(filters, kernel_size, padding='same', activation='relu')(x)\n",
    "        x = tf.keras.layers.MaxPooling1D(pool_size=2, padding='same')(x)\n",
    "\n",
    "    # Bottleneck\n",
    "    x = tf.keras.layers.Conv1D(filters, 3, padding='same', activation='relu')(x)\n",
    "\n",
    "    # Decoder (mirrors encoder)\n",
    "    for i in reversed(range(num_layers)):\n",
    "        x = tf.keras.layers.UpSampling1D(2)(x)\n",
    "        x = tf.keras.layers.Conv1D(filter_list[i], 3, padding='same', activation='relu')(x)\n",
    "\n",
    "    # Output layer\n",
    "    outputs = tf.keras.layers.Conv1D(1, 3, padding='same', activation='tanh')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    learning_rate = hp.Float('learning_rate', 1e-5, 1e-3, sampling='log')\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss='mse')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Step 5: Keras Tuner setup\n",
    "tuner = kt.RandomSearch(\n",
    "    build_autoencoder,\n",
    "    objective='val_loss',\n",
    "    max_trials=3,\n",
    "    directory='autoencoder_tuning',\n",
    "    project_name='audio_denoiser_v2'\n",
    ")\n",
    "\n",
    "# Step 6: Search for best hyperparameters\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "tuner.search(\n",
    "    noisy_signals, clean_signals,\n",
    "    epochs=10,\n",
    "    batch_size=16,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "# Step 7: Get the best model\n",
    "best_hp = tuner.get_best_hyperparameters(1)[0]\n",
    "print(\"Best Hyperparameters:\", best_hp.values)\n",
    "\n",
    "best_model = tuner.hypermodel.build(best_hp)\n",
    "best_model.fit(\n",
    "    noisy_signals, clean_signals,\n",
    "    epochs=10,\n",
    "    batch_size=16,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stop]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_9\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_9\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_72 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_73 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,352</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_74 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">20,544</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_75 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,352</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ up_sampling1d_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_76 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,352</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ up_sampling1d_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_77 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,352</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ up_sampling1d_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_78 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,352</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_79 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">193</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_9 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32000\u001b[0m, \u001b[38;5;34m1\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_72 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32000\u001b[0m, \u001b[38;5;34m64\u001b[0m)      │           \u001b[38;5;34m256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_27 (\u001b[38;5;33mMaxPooling1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16000\u001b[0m, \u001b[38;5;34m64\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_73 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16000\u001b[0m, \u001b[38;5;34m64\u001b[0m)      │        \u001b[38;5;34m12,352\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_28 (\u001b[38;5;33mMaxPooling1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8000\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_74 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8000\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │        \u001b[38;5;34m20,544\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_29 (\u001b[38;5;33mMaxPooling1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4000\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_75 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4000\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │        \u001b[38;5;34m12,352\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ up_sampling1d_27 (\u001b[38;5;33mUpSampling1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8000\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_76 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8000\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │        \u001b[38;5;34m12,352\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ up_sampling1d_28 (\u001b[38;5;33mUpSampling1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16000\u001b[0m, \u001b[38;5;34m64\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_77 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16000\u001b[0m, \u001b[38;5;34m64\u001b[0m)      │        \u001b[38;5;34m12,352\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ up_sampling1d_29 (\u001b[38;5;33mUpSampling1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32000\u001b[0m, \u001b[38;5;34m64\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_78 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32000\u001b[0m, \u001b[38;5;34m64\u001b[0m)      │        \u001b[38;5;34m12,352\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_79 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32000\u001b[0m, \u001b[38;5;34m1\u001b[0m)       │           \u001b[38;5;34m193\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">82,753</span> (323.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m82,753\u001b[0m (323.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">82,753</span> (323.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m82,753\u001b[0m (323.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 588ms/step - loss: 0.0155 - val_loss: 0.0042\n",
      "Epoch 2/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 726ms/step - loss: 0.0039 - val_loss: 0.0034\n",
      "Epoch 3/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 763ms/step - loss: 0.0033 - val_loss: 0.0030\n",
      "Epoch 4/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 783ms/step - loss: 0.0029 - val_loss: 0.0028\n",
      "Epoch 5/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 842ms/step - loss: 0.0028 - val_loss: 0.0026\n",
      "Epoch 6/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 850ms/step - loss: 0.0026 - val_loss: 0.0025\n",
      "Epoch 7/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 885ms/step - loss: 0.0023 - val_loss: 0.0024\n",
      "Epoch 8/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 773ms/step - loss: 0.0024 - val_loss: 0.0023\n",
      "Epoch 9/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 725ms/step - loss: 0.0024 - val_loss: 0.0023\n",
      "Epoch 10/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 707ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 11/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 731ms/step - loss: 0.0021 - val_loss: 0.0022\n",
      "Epoch 12/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 763ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 13/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 784ms/step - loss: 0.0020 - val_loss: 0.0021\n",
      "Epoch 14/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 803ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 15/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 837ms/step - loss: 0.0018 - val_loss: 0.0021\n",
      "Epoch 16/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 805ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 17/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 813ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 18/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 812ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 19/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 834ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 20/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 801ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 21/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 783ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 22/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 781ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 23/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 792ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 24/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 807ms/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 25/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 810ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 26/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 805ms/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 27/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 815ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 28/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 911ms/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 29/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 949ms/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 30/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 830ms/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 31/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 819ms/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 32/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 824ms/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 33/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 870ms/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 34/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 813ms/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 35/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 721ms/step - loss: 0.0015 - val_loss: 0.0017\n",
      "Epoch 36/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 708ms/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 37/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 743ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 38/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 802ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 39/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 839ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 40/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 846ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 41/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 842ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 42/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 846ms/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 43/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 846ms/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 44/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 797ms/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 45/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 750ms/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 46/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 763ms/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 47/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 784ms/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 48/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 849ms/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 49/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 856ms/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 50/50\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 860ms/step - loss: 0.0016 - val_loss: 0.0016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Get best params\n",
    "best_hp = tuner.get_best_hyperparameters(1)[0]\n",
    "\n",
    "# Build final model\n",
    "final_model = build_autoencoder(best_hp)\n",
    "final_model.summary()  # Optional: show architecture\n",
    "\n",
    "# Retrain on full data\n",
    "final_model.fit(\n",
    "    noisy_signals, clean_signals,\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "# Save final model\n",
    "final_model.save(\"audio_denoiser_best_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "\n",
    "model = load_model(\"audio_denoiser_best_model.h5\", custom_objects={'mse': MeanSquaredError()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf  # Recommended for saving audio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Denoised audio saved to: chunked_denoised_audio.wav\n"
     ]
    }
   ],
   "source": [
    "def preprocess_audio(file_path, target_sample_rate=32000):\n",
    "    audio, sr = librosa.load(file_path, sr=target_sample_rate)\n",
    "    audio = librosa.util.normalize(audio)\n",
    "    return audio\n",
    "\n",
    "def chunk_audio(audio, chunk_size=32000):\n",
    "    # Split into non-overlapping chunks of chunk_size\n",
    "    num_chunks = int(np.ceil(len(audio) / chunk_size))\n",
    "    padded_audio = np.pad(audio, (0, num_chunks * chunk_size - len(audio)), 'constant')\n",
    "    chunks = np.reshape(padded_audio, (num_chunks, chunk_size))\n",
    "    return chunks\n",
    "\n",
    "def denoise_audio(input_file, output_file, model, target_sample_rate=32000):\n",
    "    # Step 1: Preprocess the noisy input (normalize & load)\n",
    "    audio = preprocess_audio(input_file, target_sample_rate)\n",
    "    \n",
    "    # Step 2: Split into 1-sec chunks (32000 samples)\n",
    "    chunks = chunk_audio(audio, chunk_size=32000)\n",
    "    \n",
    "    denoised_chunks = []\n",
    "    for chunk in chunks:\n",
    "        chunk_input = np.expand_dims(chunk, axis=(0, -1))  # Shape: (1, 32000, 1)\n",
    "        denoised_chunk = model.predict(chunk_input)\n",
    "        denoised_chunk = denoised_chunk.squeeze()\n",
    "        denoised_chunks.append(denoised_chunk)\n",
    "    \n",
    "    # Step 3: Concatenate all denoised chunks\n",
    "    denoised_audio = np.concatenate(denoised_chunks)\n",
    "    denoised_audio = librosa.util.normalize(denoised_audio)\n",
    "    \n",
    "    # Step 4: Save the denoised audio\n",
    "    sf.write(output_file, denoised_audio, target_sample_rate)\n",
    "    print(f\"Denoised audio saved to: {output_file}\")\n",
    "\n",
    "# Example usage:\n",
    "input_wav = \"noisy_testset_wav/p232_010.wav\"\n",
    "output_wav = \"chunked_denoised_audio.wav\"\n",
    "\n",
    "denoise_audio(input_wav, output_wav, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
